from dotenv import load_dotenv, find_dotenv
from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings
from langchain.vectorstores import FAISS
import os

_ = load_dotenv(find_dotenv())
HuggingFaceInferenceAPIKey = os.environ['HUGGINGFACEHUB_API_TOKEN']

def generate_response(vector_db, user_query, llm):
    """
    This function generates a response based on a given language model (LLM) and query.
    The function takes an LLM and a query as input, sends the query to the LLM,
    and extracts the response from the LLM's output.

    Parameters:
    llm (HuggingFaceHub): The language model to be used for generating the response.
        The LLM should be initialized with a suitable model and parameters.
    query (str): The query to be sent to the LLM. The query should be a string
        containing the user's question or instruction.

    Returns:
    str: The response generated by the LLM. The response should be a string
        containing the LLM's answer to the user's question or fulfillment of the user's instruction.
    """
    context = retrieve_build_context(vector_db, user_query)
    final_query = build_query(context, user_query)
    response = llm_call(llm, final_query)
    return response

def build_query(context, user_query):
    
    prompt = f'''Act as Shubham Rajan Naik, a professional specializing in machine learning, generative AI, and software engineering. Given the context, respond directly to recruiter queries in the first person, as though I am speaking directly to them.

### Context:
{context}

### Approach:
Thoroughly evaluate the context to extract relevant details about my professional background, skills, and experience. Respond in a natural, concise, and relevant manner that directly addresses the query without introductory statements about my expertise.

### Response Format:
Provide a direct, confident answer, focusing solely on the question. Maintain a personal, first-person tone to create a natural interaction, and omit introductory or background information unless specifically asked for.

### Instructions:
- Only share details available in the context. If information to answer a query is missing, respond briefly, without extra explanation, by stating that itâ€™s unavailable.
- Keep answers straightforward, focused, and concise to match the recruiter's query closely.

### User Query:
{user_query}'''
    
    return prompt

def retrieve_build_context(vector_db, user_query):
    """
    Retrieves and builds a contextual background for a given user query using a vector database.

    This function performs a maximal marginal relevance search on the vector database to find the most relevant documents
    based on the user query. It then extracts the content of these documents and concatenates them to form a contextual
    background.

    Parameters:
    vector_db (Chroma or FAISS): The vector database to be used for information retrieval. The database should be
        initialized with documents containing the 'page_content' attribute.
    user_query (str): The user's query for which the contextual background needs to be retrieved.

    Returns:
    str: A concatenated string containing the content of the most relevant documents found in the vector database.
    """
    results = vector_db.max_marginal_relevance_search(user_query)
    context = ""
    for result in results:
        context += result.page_content
    return context

def llm_call(llm, prompt):
    """
    This function makes a call to the Together API to generate a response based on the given prompt.
    The function uses the meta-llama/Llama-Vision-Free model for text generation.

    Parameters:
    prompt (str): The input prompt for the text generation.

    Returns:
    None: The function prints the generated response from the Together API.

    Note:
    This function requires the 'together' and 'python-dotenv' libraries to be installed.
    It also requires a valid API key to be set in the environment variable 'TOGETHER_API_KEY'.
    """


    responses = llm.chat.completions.create(
        model="meta-llama/Llama-Vision-Free",
        messages=[{
                "role": "user",
                "content": prompt
            }],
        max_tokens=1512,
        temperature=0.7,
        top_p=0.7,
        top_k=50,
        repetition_penalty=1,
        stop=["<|eot_id|>","<|eom_id|>"],
        truncate=130560
    )
    return responses.choices[0].message.content

def load_vector_db(chunks):
    """
    This function initializes and loads a vector database (Chroma) with text chunks.
    The function removes any existing Chroma directory, creates a new one, and initializes
    a Chroma vector store with the provided text chunks.

    Parameters:
    chunks (List[str]): A list of text chunks to be loaded into the vector database.

    Returns:
    Chroma: An initialized Chroma vector store with the provided text chunks.
    """

    embeddings = HuggingFaceInferenceAPIEmbeddings(
        api_key=HuggingFaceInferenceAPIKey,
        model_name="sentence-transformers/all-MiniLM-l6-v2")
    vectordb = FAISS.from_texts(texts=chunks, embedding=embeddings)

    return vectordb